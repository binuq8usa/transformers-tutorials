{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from yacs.config import CfgNode as CN\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first define the transfomer network\n",
    "\n",
    "# A transfomer has an word embedding layer, position embedding layer, series of Blocks, dropout and layerNorma.\n",
    "# Each Block is another class which CausalSelfAttention Layer, and MLP layer with two layerNorms, \n",
    "# Causal Self Attention Module and an MLP module.\n",
    "# MLP module consists of fully connected layer, projection layer, relu and drop out\n",
    "# Causal Self Attention is the multi-headed attention layer\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert (config.embed_size % config.num_heads == 0)\n",
    "        \n",
    "        # layer to get the key, query, value from a batch of inputs\n",
    "        # basically this linear layer defines the weights\n",
    "        self.c_attn = nn.Linear(config.embed_size, 3 * config.embed_size, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.embed_size, config.embed_size, bias=config.bias)\n",
    "        \n",
    "        # for regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropput)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.num_heads = config.num_heads\n",
    "        self.embed_size = config.embed_size\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # causal mask to ensure attention is only applied to the left\n",
    "        # basically creating an vector called bias which creates a lower triangular portion\n",
    "        self.register_buffer(\"bias\",\n",
    "                             torch.tril(torch.ones((config.block_size, config.block_size))).view(1,1,config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_size = x.size()\n",
    "        \n",
    "        # calculate query, key values for all heads\n",
    "        query_x, key_x, value_x = self.c_attn(x).split(self.embed_size, dim=2)\n",
    "        \n",
    "        # divide into multiple heads of the key, query and value - batch_size x seq_length x num_heads x embed_size/num_heads -> batch_size x num_heads x seq_length x embed_size / num_heads\n",
    "        query_x = query_x.view(batch_size, seq_length, self.num_heads, embed_size // self.num_heads).transpose(1,2) # \n",
    "        key_x = key_x.view(batch_size, seq_length, self.num_heads, embed_size // self.num_heads).transpose(1,2) \n",
    "        value_x = value_x.view(batch_size, seq_length, self.num_heads, embed_size // self.num_heads).transpose(1,2) \n",
    "\n",
    "        # implementation of attention\n",
    "        # query is batch_size x num_heads x seq_length x embed_size//num_heads \n",
    "        # key is als the same. \n",
    "        # if batch_size = 1, num_heads=1, \n",
    "        # seq_length refers to the nodes in query and key. More like a graph\n",
    "        # each node refers to a vector at particular position in the sequence\n",
    "        # 1 x 1 x seq_length x embed_size  * 1 x 1 x embed_size x seq_length\n",
    "        # 1 x 1 x seq_length x seq_length - \n",
    "        # the interpretation\n",
    "        # for each node in query sequence length, the attention gives the scores for the nodes in the key sequence\n",
    "        \n",
    "        att = query_x @ key_x.transpose(-2,-1) * (1.0 / math.sqrt(key_x.size(-1)))     \n",
    "        \n",
    "        # now we can introduce which instant or element in the sequenc length needs to be mask\n",
    "        # by setting self.bias[ele] = 0\n",
    "        att = att.masked_fill(self.bias[:,:,:seq_length,:seq_length] == 0, float('-inf'))\n",
    "        \n",
    "        # apply softmax - for each node in query, softmax is applied to all nodes across the keys\n",
    "        att = F.softmax(att, dim = -1)\n",
    "        \n",
    "        # attention applued to all nodes in value\n",
    "        # we get batch_size x num_heads x seq_len x seq_len * \n",
    "        # batch_size x num_heads x seq_len x embed_size//num_heads\n",
    "        y = att @ value_x \n",
    "        \n",
    "        # re-assemble all heads\n",
    "        # batch_size x num_heads x seq_len x embed_size //num_heads\n",
    "        # batch_size x seq_len x num_heads x embed_size//num_heads\n",
    "        # convert to 1D ,a dn rearrance to batch_size x seq_length x embed_size\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_length, embed_size)\n",
    "        \n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.embed_size, 4 * config.embed_size, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.embed_size, config.embed_size, bias = config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "# define block\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "        Transformer Block    \n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.embed_size)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.embed_size)\n",
    "        self.mlp = MLP(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\" BINU GPT\"\"\"  \n",
    "    \n",
    "    \"\"\"\n",
    "            Hyper parameters are \n",
    "            1. Vocabulary Size = Total number of tokens in the set\n",
    "            2. Block Size = context surrounding the current word\n",
    "            3. Embedding size = size of the vector N_E\n",
    "            4. Number of heads = number of heads in a multi-headed attention. Basically, splitting the vector into N_H heads of size N_E/N_H\n",
    "            5. Number of layers = number of blocks containing the Multi-headed attention, and MLP\n",
    "            6. Dropout Hyperparameters - for embedding layer, resdual layer and attention  \n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        \n",
    "        # setting the default \n",
    "        C = CN()\n",
    "        C.block_size = None \n",
    "        C.vocab_size = None \n",
    "        C.num_layers = 3\n",
    "        C.num_heads = 3\n",
    "        C.embed_size = 48\n",
    "        C.embed_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "    \n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        assert(config.block_size is not None)\n",
    "        assert(config.vocab_size is not None)\n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = nn.ModuleDict(\n",
    "            {\n",
    "                'wte' : nn.Embedding(config.vocab_size, config.embed_size),\n",
    "                'wpe' : nn.Embedding(config.block_size, config.embed_size),\n",
    "                'drop' : nn.Dropout(config.dropout),\n",
    "                'h' : nn.ModuleList([Block(config) for _ in range(config.num_layers)]),\n",
    "                'ln_f' : nn.LayerNorm(config.embed_size, bias = config.bias)\n",
    "            }   \n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size, bias=False)\n",
    "        \n",
    "        # initialize all weights recursively through the network\n",
    "        self.apply(self._init_weights)\n",
    "        # add a special scaled unit to the residual projections\n",
    "        for param_name, param in self.named_parameters():\n",
    "            if param_name.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(param, mean=0.0, std=0.02/math.sqrt(2 * config.num_layers))\n",
    "        \n",
    "    # function to initialize weights for a module\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "            \n",
    "    def configure_optimizers(self, train_config):\n",
    "        decay_params_set = set()\n",
    "        no_decay_params_set = set()\n",
    "        \n",
    "        allowlist_weight_modules = (torch.nn.Linear, ) # set of modules that has a weight decay parameter\n",
    "        denylist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding) # set of modules that do not have a weight decay parameter\n",
    "        \n",
    "        for module_name, module in self.named_modules():\n",
    "            for param_name, params in module.named_parameters():\n",
    "                full_param_name = f'{module_name}.{param_name}' if module_name else param_name\n",
    "                \n",
    "                # bias terms will not have weight decay regularization\n",
    "                if param_name.endswith('bias'):\n",
    "                    no_decay_params_set.add(full_param_name)\n",
    "                elif param_name.endswith('weight') and isinstance(module, allowlist_weight_modules):\n",
    "                    decay_params_set.add(full_param_name)\n",
    "                elif param_name.endswith('weight') and isinstance(module, denylist_weight_modules):\n",
    "                    no_decay_params_set.add(full_param_name)\n",
    "        \n",
    "        # validation that we considered every parameter\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        \n",
    "        # no overlap between decay and no_decay param set\n",
    "        assert ( len(decay_params_set & no_decay_params_set) == 0 )\n",
    "        \n",
    "        # no parameters were missing\n",
    "        assert ( len(param_dict) - len(decay_params_set | no_decay_params_set)  == 0)\n",
    "        \n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[param_name] for param_name in sorted(list(decay_params_set))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[param_name] for param_name in sorted(list(no_decay_params_set))], \"weight_decay\": 0.0}\n",
    "        ]\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "    \n",
    "def forward(self, inputs, targets = None):\n",
    "    device = inputs.device\n",
    "    batch_size, seq_length = inputs.size()\n",
    "    assert (seq_length <= self.block_size)\n",
    "    \n",
    "    # create a postion vector of 1 x seq_length\n",
    "    position_vec = torch.arange(0, seq_length, dtype=torch.long, device=device).unsqueeze(0) \n",
    "\n",
    "    # get the token embeddings of size batch_size x seq_length x embed_size\n",
    "    token_embeddings = self.transformer.wte(inputs)\n",
    "    pos_embeddings = self.transformer.wpe(position_vec) #  1 x seq_length x embed_size\n",
    "    x = self.transformer.drop(token_embeddings + pos_embeddings) # will broadcast across batch dimensions\n",
    "    \n",
    "    # pass it through the self attention blocks\n",
    "    for block in self.transformer.h:\n",
    "        x = block(x)\n",
    "    x = self.transformer.ln_f(x)\n",
    "    \n",
    "    # obtain logits - batch_size x seq_length x vocab_size - 3D tensor\n",
    "    logits = self.lm_head(x)\n",
    "    \n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "        # here targets would be a 2D tensor batch_size x seq_length which gives the target label. \n",
    "        # we need to reshape into a 1D vector that F.cross_entropy takes\n",
    "        loss = F.cross_entropy( logits.view(-1, logits.size(-1)) , targets.view(-1))\n",
    "    return logits, loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(self, inputs, max_new_tokens, temperature = 1.0, do_sample = False, top_k = None):\n",
    "   \"\"\"\n",
    "   Take a conditioning sequence of indices 'inputs' of size (batch_size, seq_length) and complete the sequence max_new_token times, \n",
    "   feeding the predictions back into the model each itme. The model needs to be in model.eval() model\n",
    "   \"\"\" \n",
    "   for t_idx in range(max_new_tokens):\n",
    "       # take the last 'block_size' sequence length\n",
    "       inputs_cond = inputs if inputs.size(1) <= self.block_size else inputs[:,-self.block_size:]\n",
    "       \n",
    "       # forward the model to get logits\n",
    "       logits, _ = self(inputs_cond)\n",
    "       \n",
    "       # taking the logits from the last step of the sequence\n",
    "       logits = logits[:,-1,:] / temperature\n",
    "       \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
